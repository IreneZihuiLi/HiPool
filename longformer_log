INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.3475, time = 0.70 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.8133, time = 133.05 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.5284, time = 132.01 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.5467, time = 131.57 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.0442, time = 133.13 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.8330, time = 133.11 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.4928, time = 133.05 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.3398, time = 132.49 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.2574, time = 130.71 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.3801, time = 134.11 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.6006, time = 134.22 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.2449, time = 131.11 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.2093, time = 130.56 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.5198, time = 133.08 secondes ___
INFO:root:
 ******** Running time this step..1818.3786301612854
INFO:root:
*** avg_loss : 0.59, time : ~30.0 min (1818.38 sec) ***

INFO:root:==> evaluation : avg_loss = 0.43, time : 90.26 sec

INFO:root:=====>	{'accuracy': 0.8719369894982497, 'nb exemple': 3428, 'true_prediction': 2989, 'false_prediction': 439}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 2 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0671, time = 0.34 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.3485, time = 131.08 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.4917, time = 130.61 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.2485, time = 132.50 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.2615, time = 134.00 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.1144, time = 133.97 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.4509, time = 134.10 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.3068, time = 134.08 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.3356, time = 134.12 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.3134, time = 131.27 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.3844, time = 130.86 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.2159, time = 130.68 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.2628, time = 132.50 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.1811, time = 134.04 secondes ___
INFO:root:
 ******** Running time this step..1818.3548443317413
INFO:root:
*** avg_loss : 0.41, time : ~30.0 min (1818.35 sec) ***

INFO:root:==> evaluation : avg_loss = 0.41, time : 90.26 sec

INFO:root:=====>	{'accuracy': 0.8847724620770129, 'nb exemple': 3428, 'true_prediction': 3033, 'false_prediction': 395}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 3 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0540, time = 0.28 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.6057, time = 134.04 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.4725, time = 133.94 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.3289, time = 133.94 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.4131, time = 133.55 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.3053, time = 130.82 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.0236, time = 134.04 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.0330, time = 134.12 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.5052, time = 133.34 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.3020, time = 133.15 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.2739, time = 134.16 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.8219, time = 131.98 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.5063, time = 131.91 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.3406, time = 133.68 secondes ___
INFO:root:
 ******** Running time this step..1828.3446626663208
INFO:root:
*** avg_loss : 0.34, time : ~30.0 min (1828.34 sec) ***

INFO:root:==> evaluation : avg_loss = 0.46, time : 90.22 sec

INFO:root:=====>	{'accuracy': 0.8687281213535589, 'nb exemple': 3428, 'true_prediction': 2978, 'false_prediction': 450}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 4 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0321, time = 0.28 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.1055, time = 130.79 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.2847, time = 130.95 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.3347, time = 130.46 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.7366, time = 131.20 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.5952, time = 133.36 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.4140, time = 133.04 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.6199, time = 130.37 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.4672, time = 130.59 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.1724, time = 131.67 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.2238, time = 133.21 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.5087, time = 133.50 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.4215, time = 133.97 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.1395, time = 130.77 secondes ___
INFO:root:
 ******** Running time this step..1809.6880631446838
INFO:root:
*** avg_loss : 0.29, time : ~30.0 min (1809.69 sec) ***

INFO:root:==> evaluation : avg_loss = 0.39, time : 90.18 sec

INFO:root:=====>	{'accuracy': 0.8949824970828472, 'nb exemple': 3428, 'true_prediction': 3068, 'false_prediction': 360}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 5 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.7152, time = 0.28 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.0453, time = 132.98 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.2470, time = 131.84 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.1790, time = 130.92 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.0480, time = 130.99 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.1320, time = 130.73 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.2339, time = 130.66 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.0593, time = 134.22 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.5912, time = 133.94 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.2016, time = 130.62 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.0465, time = 133.97 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.3984, time = 130.97 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.0753, time = 132.99 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.0920, time = 134.12 secondes ___
INFO:root:
 ******** Running time this step..1813.2331802845001
INFO:root:
*** avg_loss : 0.26, time : ~30.0 min (1813.23 sec) ***

INFO:root:==> evaluation : avg_loss = 0.39, time : 90.27 sec

INFO:root:=====>	{'accuracy': 0.8976079346557759, 'nb exemple': 3428, 'true_prediction': 3077, 'false_prediction': 351}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 6 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0155, time = 0.34 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.1594, time = 130.95 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.7055, time = 134.09 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.1708, time = 132.52 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.1280, time = 132.48 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.2176, time = 134.14 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.2988, time = 131.10 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.0505, time = 134.20 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.2314, time = 130.96 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.5356, time = 131.03 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.1333, time = 133.10 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.4744, time = 133.54 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.1557, time = 131.87 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.2315, time = 131.92 secondes ___
INFO:root:
 ******** Running time this step..1816.8687798976898
INFO:root:
*** avg_loss : 0.23, time : ~30.0 min (1816.87 sec) ***

INFO:root:==> evaluation : avg_loss = 0.43, time : 90.24 sec

INFO:root:=====>	{'accuracy': 0.8801050175029171, 'nb exemple': 3428, 'true_prediction': 3017, 'false_prediction': 411}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 7 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0139, time = 0.28 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.6006, time = 133.26 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.5713, time = 132.60 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.3132, time = 131.13 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.1724, time = 130.88 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.3260, time = 131.77 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.0202, time = 131.71 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.5551, time = 130.72 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.4138, time = 132.52 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.1282, time = 133.11 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.0269, time = 133.19 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.3333, time = 131.10 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.2027, time = 133.30 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.2057, time = 132.56 secondes ___
INFO:root:
 ******** Running time this step..1812.3005874156952
INFO:root:
*** avg_loss : 0.21, time : ~30.0 min (1812.30 sec) ***

INFO:root:==> evaluation : avg_loss = 0.45, time : 90.23 sec

INFO:root:=====>	{'accuracy': 0.8929404900816803, 'nb exemple': 3428, 'true_prediction': 3061, 'false_prediction': 367}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 8 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0016, time = 0.28 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.2129, time = 134.21 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.2021, time = 134.23 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.1065, time = 134.21 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.1637, time = 133.78 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.0878, time = 132.10 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.0523, time = 130.97 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.3238, time = 131.25 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.1047, time = 130.51 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.1830, time = 133.22 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.1683, time = 134.09 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.0149, time = 133.79 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.0345, time = 133.88 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.1114, time = 131.40 secondes ___
INFO:root:
 ******** Running time this step..1823.1618752479553
INFO:root:
*** avg_loss : 0.18, time : ~30.0 min (1823.16 sec) ***

INFO:root:==> evaluation : avg_loss = 0.42, time : 90.23 sec

INFO:root:=====>	{'accuracy': 0.8941073512252042, 'nb exemple': 3428, 'true_prediction': 3065, 'false_prediction': 363}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 9 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.2362, time = 0.28 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.0947, time = 133.96 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.1509, time = 132.13 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.1860, time = 131.67 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.0125, time = 131.95 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.0339, time = 130.68 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.2958, time = 130.47 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.0732, time = 130.75 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.1496, time = 130.48 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.2982, time = 133.26 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.1997, time = 130.54 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.0913, time = 131.26 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.0780, time = 130.64 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.0282, time = 133.15 secondes ___
INFO:root:
 ******** Running time this step..1806.6326744556427
INFO:root:
*** avg_loss : 0.16, time : ~30.0 min (1806.63 sec) ***

INFO:root:==> evaluation : avg_loss = 0.47, time : 90.15 sec

INFO:root:=====>	{'accuracy': 0.8879813302217037, 'nb exemple': 3428, 'true_prediction': 3044, 'false_prediction': 384}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 10 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0149, time = 0.28 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.0120, time = 134.00 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.0368, time = 134.05 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.0114, time = 133.34 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.1671, time = 131.08 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.0506, time = 130.18 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.1210, time = 133.99 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.0250, time = 134.14 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.0688, time = 133.97 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.2459, time = 134.05 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.1578, time = 133.99 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.0340, time = 134.02 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.0450, time = 133.93 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.1657, time = 134.07 secondes ___
INFO:root:
 ******** Running time this step..1830.6149289608002
INFO:root:
*** avg_loss : 0.16, time : ~30.0 min (1830.62 sec) ***

INFO:root:==> evaluation : avg_loss = 0.47, time : 90.48 sec

INFO:root:=====>	{'accuracy': 0.8926487747957993, 'nb exemple': 3428, 'true_prediction': 3060, 'false_prediction': 368}
INFO:root:	§§ model has been saved §§
INFO:root:

$$$$ average running time per epoch (sec)..1817.757941555977
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.0.attention.self.query_global.weight', 'hieformer.encoder.layer.0.attention.self.query_global.bias', 'hieformer.encoder.layer.0.attention.self.key_global.weight', 'hieformer.encoder.layer.0.attention.self.key_global.bias', 'hieformer.encoder.layer.0.attention.self.value_global.weight', 'hieformer.encoder.layer.0.attention.self.value_global.bias', 'hieformer.encoder.layer.1.attention.self.query_global.weight', 'hieformer.encoder.layer.1.attention.self.query_global.bias', 'hieformer.encoder.layer.1.attention.self.key_global.weight', 'hieformer.encoder.layer.1.attention.self.key_global.bias', 'hieformer.encoder.layer.1.attention.self.value_global.weight', 'hieformer.encoder.layer.1.attention.self.value_global.bias', 'hieformer.encoder.layer.2.attention.self.query_global.weight', 'hieformer.encoder.layer.2.attention.self.query_global.bias', 'hieformer.encoder.layer.2.attention.self.key_global.weight', 'hieformer.encoder.layer.2.attention.self.key_global.bias', 'hieformer.encoder.layer.2.attention.self.value_global.weight', 'hieformer.encoder.layer.2.attention.self.value_global.bias', 'hieformer.encoder.layer.3.attention.self.query_global.weight', 'hieformer.encoder.layer.3.attention.self.query_global.bias', 'hieformer.encoder.layer.3.attention.self.key_global.weight', 'hieformer.encoder.layer.3.attention.self.key_global.bias', 'hieformer.encoder.layer.3.attention.self.value_global.weight', 'hieformer.encoder.layer.3.attention.self.value_global.bias', 'hieformer.encoder.layer.4.attention.self.query_global.weight', 'hieformer.encoder.layer.4.attention.self.query_global.bias', 'hieformer.encoder.layer.4.attention.self.key_global.weight', 'hieformer.encoder.layer.4.attention.self.key_global.bias', 'hieformer.encoder.layer.4.attention.self.value_global.weight', 'hieformer.encoder.layer.4.attention.self.value_global.bias', 'hieformer.encoder.layer.5.attention.self.query_global.weight', 'hieformer.encoder.layer.5.attention.self.query_global.bias', 'hieformer.encoder.layer.5.attention.self.key_global.weight', 'hieformer.encoder.layer.5.attention.self.key_global.bias', 'hieformer.encoder.layer.5.attention.self.value_global.weight', 'hieformer.encoder.layer.5.attention.self.value_global.bias', 'hieformer.encoder.layer.6.attention.self.query_global.weight', 'hieformer.encoder.layer.6.attention.self.query_global.bias', 'hieformer.encoder.layer.6.attention.self.key_global.weight', 'hieformer.encoder.layer.6.attention.self.key_global.bias', 'hieformer.encoder.layer.6.attention.self.value_global.weight', 'hieformer.encoder.layer.6.attention.self.value_global.bias', 'hieformer.encoder.layer.7.attention.self.query_global.weight', 'hieformer.encoder.layer.7.attention.self.query_global.bias', 'hieformer.encoder.layer.7.attention.self.key_global.weight', 'hieformer.encoder.layer.7.attention.self.key_global.bias', 'hieformer.encoder.layer.7.attention.self.value_global.weight', 'hieformer.encoder.layer.7.attention.self.value_global.bias', 'hieformer.encoder.layer.8.attention.self.query_global.weight', 'hieformer.encoder.layer.8.attention.self.query_global.bias', 'hieformer.encoder.layer.8.attention.self.key_global.weight', 'hieformer.encoder.layer.8.attention.self.key_global.bias', 'hieformer.encoder.layer.8.attention.self.value_global.weight', 'hieformer.encoder.layer.8.attention.self.value_global.bias', 'hieformer.encoder.layer.9.attention.self.query_global.weight', 'hieformer.encoder.layer.9.attention.self.query_global.bias', 'hieformer.encoder.layer.9.attention.self.key_global.weight', 'hieformer.encoder.layer.9.attention.self.key_global.bias', 'hieformer.encoder.layer.9.attention.self.value_global.weight', 'hieformer.encoder.layer.9.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.11.attention.self.query_global.weight', 'hieformer.encoder.layer.11.attention.self.query_global.bias', 'hieformer.encoder.layer.11.attention.self.key_global.weight', 'hieformer.encoder.layer.11.attention.self.key_global.bias', 'hieformer.encoder.layer.11.attention.self.value_global.weight', 'hieformer.encoder.layer.11.attention.self.value_global.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.0.attention.self.query_global.weight', 'hieformer.encoder.layer.0.attention.self.query_global.bias', 'hieformer.encoder.layer.0.attention.self.key_global.weight', 'hieformer.encoder.layer.0.attention.self.key_global.bias', 'hieformer.encoder.layer.0.attention.self.value_global.weight', 'hieformer.encoder.layer.0.attention.self.value_global.bias', 'hieformer.encoder.layer.1.attention.self.query_global.weight', 'hieformer.encoder.layer.1.attention.self.query_global.bias', 'hieformer.encoder.layer.1.attention.self.key_global.weight', 'hieformer.encoder.layer.1.attention.self.key_global.bias', 'hieformer.encoder.layer.1.attention.self.value_global.weight', 'hieformer.encoder.layer.1.attention.self.value_global.bias', 'hieformer.encoder.layer.2.attention.self.query_global.weight', 'hieformer.encoder.layer.2.attention.self.query_global.bias', 'hieformer.encoder.layer.2.attention.self.key_global.weight', 'hieformer.encoder.layer.2.attention.self.key_global.bias', 'hieformer.encoder.layer.2.attention.self.value_global.weight', 'hieformer.encoder.layer.2.attention.self.value_global.bias', 'hieformer.encoder.layer.3.attention.self.query_global.weight', 'hieformer.encoder.layer.3.attention.self.query_global.bias', 'hieformer.encoder.layer.3.attention.self.key_global.weight', 'hieformer.encoder.layer.3.attention.self.key_global.bias', 'hieformer.encoder.layer.3.attention.self.value_global.weight', 'hieformer.encoder.layer.3.attention.self.value_global.bias', 'hieformer.encoder.layer.4.attention.self.query_global.weight', 'hieformer.encoder.layer.4.attention.self.query_global.bias', 'hieformer.encoder.layer.4.attention.self.key_global.weight', 'hieformer.encoder.layer.4.attention.self.key_global.bias', 'hieformer.encoder.layer.4.attention.self.value_global.weight', 'hieformer.encoder.layer.4.attention.self.value_global.bias', 'hieformer.encoder.layer.5.attention.self.query_global.weight', 'hieformer.encoder.layer.5.attention.self.query_global.bias', 'hieformer.encoder.layer.5.attention.self.key_global.weight', 'hieformer.encoder.layer.5.attention.self.key_global.bias', 'hieformer.encoder.layer.5.attention.self.value_global.weight', 'hieformer.encoder.layer.5.attention.self.value_global.bias', 'hieformer.encoder.layer.6.attention.self.query_global.weight', 'hieformer.encoder.layer.6.attention.self.query_global.bias', 'hieformer.encoder.layer.6.attention.self.key_global.weight', 'hieformer.encoder.layer.6.attention.self.key_global.bias', 'hieformer.encoder.layer.6.attention.self.value_global.weight', 'hieformer.encoder.layer.6.attention.self.value_global.bias', 'hieformer.encoder.layer.7.attention.self.query_global.weight', 'hieformer.encoder.layer.7.attention.self.query_global.bias', 'hieformer.encoder.layer.7.attention.self.key_global.weight', 'hieformer.encoder.layer.7.attention.self.key_global.bias', 'hieformer.encoder.layer.7.attention.self.value_global.weight', 'hieformer.encoder.layer.7.attention.self.value_global.bias', 'hieformer.encoder.layer.8.attention.self.query_global.weight', 'hieformer.encoder.layer.8.attention.self.query_global.bias', 'hieformer.encoder.layer.8.attention.self.key_global.weight', 'hieformer.encoder.layer.8.attention.self.key_global.bias', 'hieformer.encoder.layer.8.attention.self.value_global.weight', 'hieformer.encoder.layer.8.attention.self.value_global.bias', 'hieformer.encoder.layer.9.attention.self.query_global.weight', 'hieformer.encoder.layer.9.attention.self.query_global.bias', 'hieformer.encoder.layer.9.attention.self.key_global.weight', 'hieformer.encoder.layer.9.attention.self.key_global.bias', 'hieformer.encoder.layer.9.attention.self.value_global.weight', 'hieformer.encoder.layer.9.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.11.attention.self.query_global.weight', 'hieformer.encoder.layer.11.attention.self.query_global.bias', 'hieformer.encoder.layer.11.attention.self.key_global.weight', 'hieformer.encoder.layer.11.attention.self.key_global.bias', 'hieformer.encoder.layer.11.attention.self.value_global.weight', 'hieformer.encoder.layer.11.attention.self.value_global.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.10.attention.self.query.weight', 'hieformer.encoder.layer.10.attention.self.query.bias', 'hieformer.encoder.layer.10.attention.self.key.weight', 'hieformer.encoder.layer.10.attention.self.key.bias', 'hieformer.encoder.layer.10.attention.self.value.weight', 'hieformer.encoder.layer.10.attention.self.value.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.output.dense.weight', 'hieformer.encoder.layer.10.attention.output.dense.bias', 'hieformer.encoder.layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.10.intermediate.dense.weight', 'hieformer.encoder.layer.10.intermediate.dense.bias', 'hieformer.encoder.layer.10.output.dense.weight', 'hieformer.encoder.layer.10.output.dense.bias', 'hieformer.encoder.layer.10.output.LayerNorm.weight', 'hieformer.encoder.layer.10.output.LayerNorm.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.10.reduction.weight', 'hieformer.encoder.layer.10.norm.weight', 'hieformer.encoder.layer.10.norm.bias', 'hieformer.encoder.layer.12.reduction.weight', 'hieformer.encoder.layer.12.norm.weight', 'hieformer.encoder.layer.12.norm.bias', 'hieformer.encoder.layer.13.attention.self.query.weight', 'hieformer.encoder.layer.13.attention.self.query.bias', 'hieformer.encoder.layer.13.attention.self.key.weight', 'hieformer.encoder.layer.13.attention.self.key.bias', 'hieformer.encoder.layer.13.attention.self.value.weight', 'hieformer.encoder.layer.13.attention.self.value.bias', 'hieformer.encoder.layer.13.attention.self.query_global.weight', 'hieformer.encoder.layer.13.attention.self.query_global.bias', 'hieformer.encoder.layer.13.attention.self.key_global.weight', 'hieformer.encoder.layer.13.attention.self.key_global.bias', 'hieformer.encoder.layer.13.attention.self.value_global.weight', 'hieformer.encoder.layer.13.attention.self.value_global.bias', 'hieformer.encoder.layer.13.attention.output.dense.weight', 'hieformer.encoder.layer.13.attention.output.dense.bias', 'hieformer.encoder.layer.13.attention.output.LayerNorm.weight', 'hieformer.encoder.layer.13.attention.output.LayerNorm.bias', 'hieformer.encoder.layer.13.intermediate.dense.weight', 'hieformer.encoder.layer.13.intermediate.dense.bias', 'hieformer.encoder.layer.13.output.dense.weight', 'hieformer.encoder.layer.13.output.dense.bias', 'hieformer.encoder.layer.13.output.LayerNorm.weight', 'hieformer.encoder.layer.13.output.LayerNorm.bias', 'hieformer.encoder.layer.14.reduction.weight', 'hieformer.encoder.layer.14.norm.weight', 'hieformer.encoder.layer.14.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'hieformer.encoder.layer.0.attention.self.query_global.weight', 'hieformer.encoder.layer.0.attention.self.query_global.bias', 'hieformer.encoder.layer.0.attention.self.key_global.weight', 'hieformer.encoder.layer.0.attention.self.key_global.bias', 'hieformer.encoder.layer.0.attention.self.value_global.weight', 'hieformer.encoder.layer.0.attention.self.value_global.bias', 'hieformer.encoder.layer.1.attention.self.query_global.weight', 'hieformer.encoder.layer.1.attention.self.query_global.bias', 'hieformer.encoder.layer.1.attention.self.key_global.weight', 'hieformer.encoder.layer.1.attention.self.key_global.bias', 'hieformer.encoder.layer.1.attention.self.value_global.weight', 'hieformer.encoder.layer.1.attention.self.value_global.bias', 'hieformer.encoder.layer.2.attention.self.query_global.weight', 'hieformer.encoder.layer.2.attention.self.query_global.bias', 'hieformer.encoder.layer.2.attention.self.key_global.weight', 'hieformer.encoder.layer.2.attention.self.key_global.bias', 'hieformer.encoder.layer.2.attention.self.value_global.weight', 'hieformer.encoder.layer.2.attention.self.value_global.bias', 'hieformer.encoder.layer.3.attention.self.query_global.weight', 'hieformer.encoder.layer.3.attention.self.query_global.bias', 'hieformer.encoder.layer.3.attention.self.key_global.weight', 'hieformer.encoder.layer.3.attention.self.key_global.bias', 'hieformer.encoder.layer.3.attention.self.value_global.weight', 'hieformer.encoder.layer.3.attention.self.value_global.bias', 'hieformer.encoder.layer.4.attention.self.query_global.weight', 'hieformer.encoder.layer.4.attention.self.query_global.bias', 'hieformer.encoder.layer.4.attention.self.key_global.weight', 'hieformer.encoder.layer.4.attention.self.key_global.bias', 'hieformer.encoder.layer.4.attention.self.value_global.weight', 'hieformer.encoder.layer.4.attention.self.value_global.bias', 'hieformer.encoder.layer.5.attention.self.query_global.weight', 'hieformer.encoder.layer.5.attention.self.query_global.bias', 'hieformer.encoder.layer.5.attention.self.key_global.weight', 'hieformer.encoder.layer.5.attention.self.key_global.bias', 'hieformer.encoder.layer.5.attention.self.value_global.weight', 'hieformer.encoder.layer.5.attention.self.value_global.bias', 'hieformer.encoder.layer.6.attention.self.query_global.weight', 'hieformer.encoder.layer.6.attention.self.query_global.bias', 'hieformer.encoder.layer.6.attention.self.key_global.weight', 'hieformer.encoder.layer.6.attention.self.key_global.bias', 'hieformer.encoder.layer.6.attention.self.value_global.weight', 'hieformer.encoder.layer.6.attention.self.value_global.bias', 'hieformer.encoder.layer.7.attention.self.query_global.weight', 'hieformer.encoder.layer.7.attention.self.query_global.bias', 'hieformer.encoder.layer.7.attention.self.key_global.weight', 'hieformer.encoder.layer.7.attention.self.key_global.bias', 'hieformer.encoder.layer.7.attention.self.value_global.weight', 'hieformer.encoder.layer.7.attention.self.value_global.bias', 'hieformer.encoder.layer.8.attention.self.query_global.weight', 'hieformer.encoder.layer.8.attention.self.query_global.bias', 'hieformer.encoder.layer.8.attention.self.key_global.weight', 'hieformer.encoder.layer.8.attention.self.key_global.bias', 'hieformer.encoder.layer.8.attention.self.value_global.weight', 'hieformer.encoder.layer.8.attention.self.value_global.bias', 'hieformer.encoder.layer.9.attention.self.query_global.weight', 'hieformer.encoder.layer.9.attention.self.query_global.bias', 'hieformer.encoder.layer.9.attention.self.key_global.weight', 'hieformer.encoder.layer.9.attention.self.key_global.bias', 'hieformer.encoder.layer.9.attention.self.value_global.weight', 'hieformer.encoder.layer.9.attention.self.value_global.bias', 'hieformer.encoder.layer.10.attention.self.query_global.weight', 'hieformer.encoder.layer.10.attention.self.query_global.bias', 'hieformer.encoder.layer.10.attention.self.key_global.weight', 'hieformer.encoder.layer.10.attention.self.key_global.bias', 'hieformer.encoder.layer.10.attention.self.value_global.weight', 'hieformer.encoder.layer.10.attention.self.value_global.bias', 'hieformer.encoder.layer.11.attention.self.query_global.weight', 'hieformer.encoder.layer.11.attention.self.query_global.bias', 'hieformer.encoder.layer.11.attention.self.key_global.weight', 'hieformer.encoder.layer.11.attention.self.key_global.bias', 'hieformer.encoder.layer.11.attention.self.value_global.weight', 'hieformer.encoder.layer.11.attention.self.value_global.bias']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.att_layer.0.attention.self.query.weight', 'hieformer.encoder.att_layer.0.attention.self.query.bias', 'hieformer.encoder.att_layer.0.attention.self.key.weight', 'hieformer.encoder.att_layer.0.attention.self.key.bias', 'hieformer.encoder.att_layer.0.attention.self.value.weight', 'hieformer.encoder.att_layer.0.attention.self.value.bias', 'hieformer.encoder.att_layer.0.attention.self.query_global.weight', 'hieformer.encoder.att_layer.0.attention.self.query_global.bias', 'hieformer.encoder.att_layer.0.attention.self.key_global.weight', 'hieformer.encoder.att_layer.0.attention.self.key_global.bias', 'hieformer.encoder.att_layer.0.attention.self.value_global.weight', 'hieformer.encoder.att_layer.0.attention.self.value_global.bias', 'hieformer.encoder.att_layer.0.attention.output.dense.weight', 'hieformer.encoder.att_layer.0.attention.output.dense.bias', 'hieformer.encoder.att_layer.0.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.0.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.0.intermediate.dense.weight', 'hieformer.encoder.att_layer.0.intermediate.dense.bias', 'hieformer.encoder.att_layer.0.output.dense.weight', 'hieformer.encoder.att_layer.0.output.dense.bias', 'hieformer.encoder.att_layer.0.output.LayerNorm.weight', 'hieformer.encoder.att_layer.0.output.LayerNorm.bias', 'hieformer.encoder.att_layer.1.attention.self.query.weight', 'hieformer.encoder.att_layer.1.attention.self.query.bias', 'hieformer.encoder.att_layer.1.attention.self.key.weight', 'hieformer.encoder.att_layer.1.attention.self.key.bias', 'hieformer.encoder.att_layer.1.attention.self.value.weight', 'hieformer.encoder.att_layer.1.attention.self.value.bias', 'hieformer.encoder.att_layer.1.attention.self.query_global.weight', 'hieformer.encoder.att_layer.1.attention.self.query_global.bias', 'hieformer.encoder.att_layer.1.attention.self.key_global.weight', 'hieformer.encoder.att_layer.1.attention.self.key_global.bias', 'hieformer.encoder.att_layer.1.attention.self.value_global.weight', 'hieformer.encoder.att_layer.1.attention.self.value_global.bias', 'hieformer.encoder.att_layer.1.attention.output.dense.weight', 'hieformer.encoder.att_layer.1.attention.output.dense.bias', 'hieformer.encoder.att_layer.1.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.1.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.1.intermediate.dense.weight', 'hieformer.encoder.att_layer.1.intermediate.dense.bias', 'hieformer.encoder.att_layer.1.output.dense.weight', 'hieformer.encoder.att_layer.1.output.dense.bias', 'hieformer.encoder.att_layer.1.output.LayerNorm.weight', 'hieformer.encoder.att_layer.1.output.LayerNorm.bias', 'hieformer.encoder.att_layer.2.attention.self.query.weight', 'hieformer.encoder.att_layer.2.attention.self.query.bias', 'hieformer.encoder.att_layer.2.attention.self.key.weight', 'hieformer.encoder.att_layer.2.attention.self.key.bias', 'hieformer.encoder.att_layer.2.attention.self.value.weight', 'hieformer.encoder.att_layer.2.attention.self.value.bias', 'hieformer.encoder.att_layer.2.attention.self.query_global.weight', 'hieformer.encoder.att_layer.2.attention.self.query_global.bias', 'hieformer.encoder.att_layer.2.attention.self.key_global.weight', 'hieformer.encoder.att_layer.2.attention.self.key_global.bias', 'hieformer.encoder.att_layer.2.attention.self.value_global.weight', 'hieformer.encoder.att_layer.2.attention.self.value_global.bias', 'hieformer.encoder.att_layer.2.attention.output.dense.weight', 'hieformer.encoder.att_layer.2.attention.output.dense.bias', 'hieformer.encoder.att_layer.2.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.2.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.2.intermediate.dense.weight', 'hieformer.encoder.att_layer.2.intermediate.dense.bias', 'hieformer.encoder.att_layer.2.output.dense.weight', 'hieformer.encoder.att_layer.2.output.dense.bias', 'hieformer.encoder.att_layer.2.output.LayerNorm.weight', 'hieformer.encoder.att_layer.2.output.LayerNorm.bias', 'hieformer.encoder.att_layer.3.attention.self.query.weight', 'hieformer.encoder.att_layer.3.attention.self.query.bias', 'hieformer.encoder.att_layer.3.attention.self.key.weight', 'hieformer.encoder.att_layer.3.attention.self.key.bias', 'hieformer.encoder.att_layer.3.attention.self.value.weight', 'hieformer.encoder.att_layer.3.attention.self.value.bias', 'hieformer.encoder.att_layer.3.attention.self.query_global.weight', 'hieformer.encoder.att_layer.3.attention.self.query_global.bias', 'hieformer.encoder.att_layer.3.attention.self.key_global.weight', 'hieformer.encoder.att_layer.3.attention.self.key_global.bias', 'hieformer.encoder.att_layer.3.attention.self.value_global.weight', 'hieformer.encoder.att_layer.3.attention.self.value_global.bias', 'hieformer.encoder.att_layer.3.attention.output.dense.weight', 'hieformer.encoder.att_layer.3.attention.output.dense.bias', 'hieformer.encoder.att_layer.3.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.3.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.3.intermediate.dense.weight', 'hieformer.encoder.att_layer.3.intermediate.dense.bias', 'hieformer.encoder.att_layer.3.output.dense.weight', 'hieformer.encoder.att_layer.3.output.dense.bias', 'hieformer.encoder.att_layer.3.output.LayerNorm.weight', 'hieformer.encoder.att_layer.3.output.LayerNorm.bias', 'hieformer.encoder.att_layer.4.attention.self.query.weight', 'hieformer.encoder.att_layer.4.attention.self.query.bias', 'hieformer.encoder.att_layer.4.attention.self.key.weight', 'hieformer.encoder.att_layer.4.attention.self.key.bias', 'hieformer.encoder.att_layer.4.attention.self.value.weight', 'hieformer.encoder.att_layer.4.attention.self.value.bias', 'hieformer.encoder.att_layer.4.attention.self.query_global.weight', 'hieformer.encoder.att_layer.4.attention.self.query_global.bias', 'hieformer.encoder.att_layer.4.attention.self.key_global.weight', 'hieformer.encoder.att_layer.4.attention.self.key_global.bias', 'hieformer.encoder.att_layer.4.attention.self.value_global.weight', 'hieformer.encoder.att_layer.4.attention.self.value_global.bias', 'hieformer.encoder.att_layer.4.attention.output.dense.weight', 'hieformer.encoder.att_layer.4.attention.output.dense.bias', 'hieformer.encoder.att_layer.4.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.4.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.4.intermediate.dense.weight', 'hieformer.encoder.att_layer.4.intermediate.dense.bias', 'hieformer.encoder.att_layer.4.output.dense.weight', 'hieformer.encoder.att_layer.4.output.dense.bias', 'hieformer.encoder.att_layer.4.output.LayerNorm.weight', 'hieformer.encoder.att_layer.4.output.LayerNorm.bias', 'hieformer.encoder.att_layer.5.attention.self.query.weight', 'hieformer.encoder.att_layer.5.attention.self.query.bias', 'hieformer.encoder.att_layer.5.attention.self.key.weight', 'hieformer.encoder.att_layer.5.attention.self.key.bias', 'hieformer.encoder.att_layer.5.attention.self.value.weight', 'hieformer.encoder.att_layer.5.attention.self.value.bias', 'hieformer.encoder.att_layer.5.attention.self.query_global.weight', 'hieformer.encoder.att_layer.5.attention.self.query_global.bias', 'hieformer.encoder.att_layer.5.attention.self.key_global.weight', 'hieformer.encoder.att_layer.5.attention.self.key_global.bias', 'hieformer.encoder.att_layer.5.attention.self.value_global.weight', 'hieformer.encoder.att_layer.5.attention.self.value_global.bias', 'hieformer.encoder.att_layer.5.attention.output.dense.weight', 'hieformer.encoder.att_layer.5.attention.output.dense.bias', 'hieformer.encoder.att_layer.5.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.5.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.5.intermediate.dense.weight', 'hieformer.encoder.att_layer.5.intermediate.dense.bias', 'hieformer.encoder.att_layer.5.output.dense.weight', 'hieformer.encoder.att_layer.5.output.dense.bias', 'hieformer.encoder.att_layer.5.output.LayerNorm.weight', 'hieformer.encoder.att_layer.5.output.LayerNorm.bias', 'hieformer.encoder.att_layer.6.attention.self.query.weight', 'hieformer.encoder.att_layer.6.attention.self.query.bias', 'hieformer.encoder.att_layer.6.attention.self.key.weight', 'hieformer.encoder.att_layer.6.attention.self.key.bias', 'hieformer.encoder.att_layer.6.attention.self.value.weight', 'hieformer.encoder.att_layer.6.attention.self.value.bias', 'hieformer.encoder.att_layer.6.attention.self.query_global.weight', 'hieformer.encoder.att_layer.6.attention.self.query_global.bias', 'hieformer.encoder.att_layer.6.attention.self.key_global.weight', 'hieformer.encoder.att_layer.6.attention.self.key_global.bias', 'hieformer.encoder.att_layer.6.attention.self.value_global.weight', 'hieformer.encoder.att_layer.6.attention.self.value_global.bias', 'hieformer.encoder.att_layer.6.attention.output.dense.weight', 'hieformer.encoder.att_layer.6.attention.output.dense.bias', 'hieformer.encoder.att_layer.6.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.6.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.6.intermediate.dense.weight', 'hieformer.encoder.att_layer.6.intermediate.dense.bias', 'hieformer.encoder.att_layer.6.output.dense.weight', 'hieformer.encoder.att_layer.6.output.dense.bias', 'hieformer.encoder.att_layer.6.output.LayerNorm.weight', 'hieformer.encoder.att_layer.6.output.LayerNorm.bias', 'hieformer.encoder.att_layer.7.attention.self.query.weight', 'hieformer.encoder.att_layer.7.attention.self.query.bias', 'hieformer.encoder.att_layer.7.attention.self.key.weight', 'hieformer.encoder.att_layer.7.attention.self.key.bias', 'hieformer.encoder.att_layer.7.attention.self.value.weight', 'hieformer.encoder.att_layer.7.attention.self.value.bias', 'hieformer.encoder.att_layer.7.attention.self.query_global.weight', 'hieformer.encoder.att_layer.7.attention.self.query_global.bias', 'hieformer.encoder.att_layer.7.attention.self.key_global.weight', 'hieformer.encoder.att_layer.7.attention.self.key_global.bias', 'hieformer.encoder.att_layer.7.attention.self.value_global.weight', 'hieformer.encoder.att_layer.7.attention.self.value_global.bias', 'hieformer.encoder.att_layer.7.attention.output.dense.weight', 'hieformer.encoder.att_layer.7.attention.output.dense.bias', 'hieformer.encoder.att_layer.7.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.7.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.7.intermediate.dense.weight', 'hieformer.encoder.att_layer.7.intermediate.dense.bias', 'hieformer.encoder.att_layer.7.output.dense.weight', 'hieformer.encoder.att_layer.7.output.dense.bias', 'hieformer.encoder.att_layer.7.output.LayerNorm.weight', 'hieformer.encoder.att_layer.7.output.LayerNorm.bias', 'hieformer.encoder.att_layer.8.attention.self.query.weight', 'hieformer.encoder.att_layer.8.attention.self.query.bias', 'hieformer.encoder.att_layer.8.attention.self.key.weight', 'hieformer.encoder.att_layer.8.attention.self.key.bias', 'hieformer.encoder.att_layer.8.attention.self.value.weight', 'hieformer.encoder.att_layer.8.attention.self.value.bias', 'hieformer.encoder.att_layer.8.attention.self.query_global.weight', 'hieformer.encoder.att_layer.8.attention.self.query_global.bias', 'hieformer.encoder.att_layer.8.attention.self.key_global.weight', 'hieformer.encoder.att_layer.8.attention.self.key_global.bias', 'hieformer.encoder.att_layer.8.attention.self.value_global.weight', 'hieformer.encoder.att_layer.8.attention.self.value_global.bias', 'hieformer.encoder.att_layer.8.attention.output.dense.weight', 'hieformer.encoder.att_layer.8.attention.output.dense.bias', 'hieformer.encoder.att_layer.8.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.8.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.8.intermediate.dense.weight', 'hieformer.encoder.att_layer.8.intermediate.dense.bias', 'hieformer.encoder.att_layer.8.output.dense.weight', 'hieformer.encoder.att_layer.8.output.dense.bias', 'hieformer.encoder.att_layer.8.output.LayerNorm.weight', 'hieformer.encoder.att_layer.8.output.LayerNorm.bias', 'hieformer.encoder.att_layer.9.attention.self.query.weight', 'hieformer.encoder.att_layer.9.attention.self.query.bias', 'hieformer.encoder.att_layer.9.attention.self.key.weight', 'hieformer.encoder.att_layer.9.attention.self.key.bias', 'hieformer.encoder.att_layer.9.attention.self.value.weight', 'hieformer.encoder.att_layer.9.attention.self.value.bias', 'hieformer.encoder.att_layer.9.attention.self.query_global.weight', 'hieformer.encoder.att_layer.9.attention.self.query_global.bias', 'hieformer.encoder.att_layer.9.attention.self.key_global.weight', 'hieformer.encoder.att_layer.9.attention.self.key_global.bias', 'hieformer.encoder.att_layer.9.attention.self.value_global.weight', 'hieformer.encoder.att_layer.9.attention.self.value_global.bias', 'hieformer.encoder.att_layer.9.attention.output.dense.weight', 'hieformer.encoder.att_layer.9.attention.output.dense.bias', 'hieformer.encoder.att_layer.9.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.9.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.9.intermediate.dense.weight', 'hieformer.encoder.att_layer.9.intermediate.dense.bias', 'hieformer.encoder.att_layer.9.output.dense.weight', 'hieformer.encoder.att_layer.9.output.dense.bias', 'hieformer.encoder.att_layer.9.output.LayerNorm.weight', 'hieformer.encoder.att_layer.9.output.LayerNorm.bias', 'hieformer.encoder.att_layer.10.attention.self.query.weight', 'hieformer.encoder.att_layer.10.attention.self.query.bias', 'hieformer.encoder.att_layer.10.attention.self.key.weight', 'hieformer.encoder.att_layer.10.attention.self.key.bias', 'hieformer.encoder.att_layer.10.attention.self.value.weight', 'hieformer.encoder.att_layer.10.attention.self.value.bias', 'hieformer.encoder.att_layer.10.attention.self.query_global.weight', 'hieformer.encoder.att_layer.10.attention.self.query_global.bias', 'hieformer.encoder.att_layer.10.attention.self.key_global.weight', 'hieformer.encoder.att_layer.10.attention.self.key_global.bias', 'hieformer.encoder.att_layer.10.attention.self.value_global.weight', 'hieformer.encoder.att_layer.10.attention.self.value_global.bias', 'hieformer.encoder.att_layer.10.attention.output.dense.weight', 'hieformer.encoder.att_layer.10.attention.output.dense.bias', 'hieformer.encoder.att_layer.10.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.10.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.10.intermediate.dense.weight', 'hieformer.encoder.att_layer.10.intermediate.dense.bias', 'hieformer.encoder.att_layer.10.output.dense.weight', 'hieformer.encoder.att_layer.10.output.dense.bias', 'hieformer.encoder.att_layer.10.output.LayerNorm.weight', 'hieformer.encoder.att_layer.10.output.LayerNorm.bias', 'hieformer.encoder.att_layer.11.attention.self.query.weight', 'hieformer.encoder.att_layer.11.attention.self.query.bias', 'hieformer.encoder.att_layer.11.attention.self.key.weight', 'hieformer.encoder.att_layer.11.attention.self.key.bias', 'hieformer.encoder.att_layer.11.attention.self.value.weight', 'hieformer.encoder.att_layer.11.attention.self.value.bias', 'hieformer.encoder.att_layer.11.attention.self.query_global.weight', 'hieformer.encoder.att_layer.11.attention.self.query_global.bias', 'hieformer.encoder.att_layer.11.attention.self.key_global.weight', 'hieformer.encoder.att_layer.11.attention.self.key_global.bias', 'hieformer.encoder.att_layer.11.attention.self.value_global.weight', 'hieformer.encoder.att_layer.11.attention.self.value_global.bias', 'hieformer.encoder.att_layer.11.attention.output.dense.weight', 'hieformer.encoder.att_layer.11.attention.output.dense.bias', 'hieformer.encoder.att_layer.11.attention.output.LayerNorm.weight', 'hieformer.encoder.att_layer.11.attention.output.LayerNorm.bias', 'hieformer.encoder.att_layer.11.intermediate.dense.weight', 'hieformer.encoder.att_layer.11.intermediate.dense.bias', 'hieformer.encoder.att_layer.11.output.dense.weight', 'hieformer.encoder.att_layer.11.output.dense.bias', 'hieformer.encoder.att_layer.11.output.LayerNorm.weight', 'hieformer.encoder.att_layer.11.output.LayerNorm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.merge_layer.0.reduction.weight', 'hieformer.encoder.merge_layer.0.norm.weight', 'hieformer.encoder.merge_layer.0.norm.bias', 'hieformer.encoder.merge_layer.1.reduction.weight', 'hieformer.encoder.merge_layer.1.norm.weight', 'hieformer.encoder.merge_layer.1.norm.bias', 'hieformer.encoder.merge_layer.2.reduction.weight', 'hieformer.encoder.merge_layer.2.norm.weight', 'hieformer.encoder.merge_layer.2.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.merge_layer.0.reduction.weight', 'hieformer.encoder.merge_layer.0.norm.weight', 'hieformer.encoder.merge_layer.0.norm.bias', 'hieformer.encoder.merge_layer.1.reduction.weight', 'hieformer.encoder.merge_layer.1.norm.weight', 'hieformer.encoder.merge_layer.1.norm.bias', 'hieformer.encoder.merge_layer.2.reduction.weight', 'hieformer.encoder.merge_layer.2.norm.weight', 'hieformer.encoder.merge_layer.2.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.merge_layer.0.reduction.weight', 'hieformer.encoder.merge_layer.0.norm.weight', 'hieformer.encoder.merge_layer.0.norm.bias', 'hieformer.encoder.merge_layer.1.reduction.weight', 'hieformer.encoder.merge_layer.1.norm.weight', 'hieformer.encoder.merge_layer.1.norm.bias', 'hieformer.encoder.merge_layer.2.reduction.weight', 'hieformer.encoder.merge_layer.2.norm.weight', 'hieformer.encoder.merge_layer.2.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.4415, time = 0.67 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.2449, time = 117.97 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.0525, time = 118.65 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.6611, time = 118.01 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.4359, time = 117.76 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.3649, time = 118.59 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.4331, time = 119.74 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.4483, time = 118.69 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.7428, time = 118.03 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.9023, time = 117.85 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.4610, time = 117.64 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.6234, time = 117.91 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.7967, time = 117.33 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.5512, time = 117.57 secondes ___
INFO:root:
 ******** Running time this step..1620.8655233383179
INFO:root:
*** avg_loss : 0.75, time : ~27.0 min (1620.87 sec) ***

INFO:root:==> evaluation : avg_loss = 0.54, time : 78.82 sec

INFO:root:=====>	{'accuracy': 0.8523920653442241, 'nb exemple': 3428, 'true_prediction': 2922, 'false_prediction': 506}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 2 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.2319, time = 0.30 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.6232, time = 117.65 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.6316, time = 117.46 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.4511, time = 119.45 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.1689, time = 118.96 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.5954, time = 117.65 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.8541, time = 118.09 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.5223, time = 119.65 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.4516, time = 119.86 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.6060, time = 116.41 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.6313, time = 115.29 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.2603, time = 115.35 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.7102, time = 115.22 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.5064, time = 114.99 secondes ___
INFO:root:
 ******** Running time this step..1609.83651471138
INFO:root:
*** avg_loss : 0.50, time : ~26.0 min (1609.84 sec) ***

INFO:root:==> evaluation : avg_loss = 0.58, time : 78.28 sec

INFO:root:=====>	{'accuracy': 0.8194282380396732, 'nb exemple': 3428, 'true_prediction': 2809, 'false_prediction': 619}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 3 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.6479, time = 0.25 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.5126, time = 116.87 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.5753, time = 115.61 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.4553, time = 116.59 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.4809, time = 116.58 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.2146, time = 116.75 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.4803, time = 116.91 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.2922, time = 115.72 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.8411, time = 116.31 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.8064, time = 116.46 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.2780, time = 116.47 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.2312, time = 116.33 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.9128, time = 116.01 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.6822, time = 114.22 secondes ___
INFO:root:
 ******** Running time this step..1592.986344575882
INFO:root:
*** avg_loss : 0.42, time : ~26.0 min (1592.99 sec) ***

INFO:root:==> evaluation : avg_loss = 0.44, time : 78.39 sec

INFO:root:=====>	{'accuracy': 0.8748541423570595, 'nb exemple': 3428, 'true_prediction': 2999, 'false_prediction': 429}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 4 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0822, time = 0.25 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.0876, time = 117.85 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.0405, time = 117.56 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.6169, time = 117.56 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.3801, time = 115.27 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.1219, time = 115.26 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.3420, time = 115.44 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.4073, time = 114.52 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.3115, time = 116.87 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.0778, time = 115.69 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.6072, time = 115.19 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.4324, time = 116.11 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.1168, time = 116.75 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.6626, time = 116.79 secondes ___
INFO:root:
 ******** Running time this step..1594.2745089530945
INFO:root:
*** avg_loss : 0.38, time : ~26.0 min (1594.27 sec) ***

INFO:root:==> evaluation : avg_loss = 0.47, time : 78.18 sec

INFO:root:=====>	{'accuracy': 0.8722287047841307, 'nb exemple': 3428, 'true_prediction': 2990, 'false_prediction': 438}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 5 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0756, time = 0.25 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.0971, time = 116.19 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.0639, time = 116.83 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.1262, time = 116.89 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.8982, time = 116.84 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.3955, time = 116.90 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.3804, time = 116.83 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.5944, time = 117.48 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.0983, time = 117.13 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.2393, time = 117.00 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.6045, time = 115.65 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.1425, time = 116.70 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.2776, time = 115.83 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.2059, time = 117.32 secondes ___
INFO:root:
 ******** Running time this step..1601.0993437767029
INFO:root:
*** avg_loss : 0.34, time : ~26.0 min (1601.10 sec) ***

INFO:root:==> evaluation : avg_loss = 0.44, time : 78.42 sec

INFO:root:=====>	{'accuracy': 0.8812718786464411, 'nb exemple': 3428, 'true_prediction': 3021, 'false_prediction': 407}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 6 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0705, time = 0.30 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.5995, time = 117.40 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.7844, time = 116.06 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.5720, time = 116.74 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.8108, time = 116.65 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.3788, time = 117.46 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.5526, time = 118.59 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.3907, time = 118.81 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.7199, time = 119.06 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.1013, time = 118.82 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.1440, time = 117.97 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.4961, time = 118.00 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.2035, time = 118.47 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.9503, time = 118.91 secondes ___
INFO:root:
 ******** Running time this step..1617.8259584903717
INFO:root:
*** avg_loss : 0.44, time : ~26.0 min (1617.83 sec) ***

INFO:root:==> evaluation : avg_loss = 0.48, time : 78.40 sec

INFO:root:=====>	{'accuracy': 0.8754375729288215, 'nb exemple': 3428, 'true_prediction': 3001, 'false_prediction': 427}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 7 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.0239, time = 0.25 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.2136, time = 117.02 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.2943, time = 117.41 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.2999, time = 117.13 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.4016, time = 117.12 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.6907, time = 117.03 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.0742, time = 115.73 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.1106, time = 115.12 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.4090, time = 114.28 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.1020, time = 114.63 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.1592, time = 115.83 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.2494, time = 116.42 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.2808, time = 117.23 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.8883, time = 117.81 secondes ___
INFO:root:
 ******** Running time this step..1596.0898962020874
INFO:root:
*** avg_loss : 0.32, time : ~26.0 min (1596.09 sec) ***

INFO:root:==> evaluation : avg_loss = 0.44, time : 78.34 sec

INFO:root:=====>	{'accuracy': 0.8838973162193698, 'nb exemple': 3428, 'true_prediction': 3030, 'false_prediction': 398}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 8 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 0.1610, time = 0.24 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.0721, time = 114.21 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.1615, time = 114.68 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.0241, time = 116.71 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.5259, time = 116.64 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.0677, time = 117.06 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.3540, time = 116.76 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.4623, time = 116.91 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.0680, time = 114.89 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.0565, time = 114.63 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.0564, time = 115.59 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.0529, time = 116.99 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.0895, time = 117.13 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.4162, time = 117.45 secondes ___
INFO:root:
 ******** Running time this step..1592.5774767398834
INFO:root:
*** avg_loss : 0.29, time : ~26.0 min (1592.58 sec) ***

INFO:root:==> evaluation : avg_loss = 0.47, time : 78.52 sec

INFO:root:=====>	{'accuracy': 0.8736872812135356, 'nb exemple': 3428, 'true_prediction': 2995, 'false_prediction': 433}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 9 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.3171, time = 0.25 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.1031, time = 116.33 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.2942, time = 117.44 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.5164, time = 117.48 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.3890, time = 115.76 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.3335, time = 116.52 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.0785, time = 117.34 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.2811, time = 117.44 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.2412, time = 117.06 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.0323, time = 116.86 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.4684, time = 116.55 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.3993, time = 116.21 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.0621, time = 116.95 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.1112, time = 117.88 secondes ___
INFO:root:
 ******** Running time this step..1603.9591183662415
INFO:root:
*** avg_loss : 0.30, time : ~26.0 min (1603.96 sec) ***

INFO:root:==> evaluation : avg_loss = 0.54, time : 78.53 sec

INFO:root:=====>	{'accuracy': 0.8582263710618436, 'nb exemple': 3428, 'true_prediction': 2942, 'false_prediction': 486}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 10 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.8008, time = 0.25 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.2456, time = 118.16 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 0.1916, time = 117.95 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 0.0287, time = 116.47 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 0.4160, time = 116.80 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 0.0989, time = 117.15 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 0.0333, time = 116.64 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 0.2815, time = 116.33 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 0.2245, time = 115.90 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 0.3249, time = 116.32 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 0.0588, time = 115.91 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 0.1154, time = 116.18 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 0.1675, time = 116.50 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 0.1480, time = 117.23 secondes ___
INFO:root:
 ******** Running time this step..1601.259978055954
INFO:root:
*** avg_loss : 0.29, time : ~26.0 min (1601.26 sec) ***

INFO:root:==> evaluation : avg_loss = 0.54, time : 78.46 sec

INFO:root:=====>	{'accuracy': 0.8663943990665111, 'nb exemple': 3428, 'true_prediction': 2970, 'false_prediction': 458}
INFO:root:	§§ model has been saved §§
INFO:root:

$$$$ average running time per epoch (sec)..1603.0775718212128
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.1635, time = 0.71 secondes ___
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.1676, time = 0.74 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.8660, time = 146.79 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 2.1692, time = 148.08 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.8763, time = 146.35 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 2.0753, time = 146.13 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.7276, time = 146.84 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.6386, time = 146.85 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.7532, time = 149.64 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.8043, time = 150.04 secondes ___
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.0.attention.self.reduction.bias', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.bias', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.bias', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.bias', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.bias', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.bias', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.bias', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.bias', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.bias', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.bias', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.bias', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.2334, time = 0.74 secondes ___
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.3843, time = 0.77 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.9956, time = 154.71 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.8686, time = 154.57 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.9471, time = 153.50 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.7009, time = 153.74 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.8600, time = 152.15 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 2.3379, time = 152.20 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.6096, time = 154.26 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.8474, time = 154.38 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.7747, time = 152.91 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.8369, time = 153.33 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 1.6855, time = 154.38 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.7800, time = 154.43 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.7311, time = 153.35 secondes ___
INFO:root:
 ******** Running time this step..2108.6151020526886
INFO:root:
*** avg_loss : 1.85, time : ~35.0 min (2108.62 sec) ***

INFO:root:==> evaluation : avg_loss = 1.82, time : 110.99 sec

INFO:root:=====>	{'accuracy': 0.35822637106184363, 'nb exemple': 3428, 'true_prediction': 1228, 'false_prediction': 2200}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 2 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.9888, time = 0.37 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.8109, time = 154.31 secondes ___
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.2354, time = 0.73 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 2.2666, time = 153.39 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.6715, time = 153.60 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.8975, time = 153.73 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.8342, time = 153.90 secondes ___
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.layer.0.attention.self.reduction.weight', 'hieformer.encoder.layer.1.attention.self.reduction.weight', 'hieformer.encoder.layer.2.attention.self.reduction.weight', 'hieformer.encoder.layer.3.attention.self.reduction.weight', 'hieformer.encoder.layer.4.attention.self.reduction.weight', 'hieformer.encoder.layer.5.attention.self.reduction.weight', 'hieformer.encoder.layer.6.attention.self.reduction.weight', 'hieformer.encoder.layer.7.attention.self.reduction.weight', 'hieformer.encoder.layer.8.attention.self.reduction.weight', 'hieformer.encoder.layer.9.attention.self.reduction.weight', 'hieformer.encoder.layer.10.attention.self.reduction.weight', 'hieformer.encoder.layer.11.attention.self.reduction.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.3481, time = 0.71 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 0.5632, time = 137.53 secondes ___
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.merge_layer.0.reduction.weight', 'hieformer.encoder.merge_layer.0.norm.weight', 'hieformer.encoder.merge_layer.0.norm.bias', 'hieformer.encoder.merge_layer.1.reduction.weight', 'hieformer.encoder.merge_layer.1.norm.weight', 'hieformer.encoder.merge_layer.1.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['hieformer.encoder.merge_layer.0.reduction.weight', 'hieformer.encoder.merge_layer.0.norm.weight', 'hieformer.encoder.merge_layer.0.norm.bias', 'hieformer.encoder.merge_layer.1.reduction.weight', 'hieformer.encoder.merge_layer.1.norm.weight', 'hieformer.encoder.merge_layer.1.norm.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.0899, time = 0.58 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.9115, time = 75.81 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.6825, time = 76.48 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.0478, time = 75.42 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.7562, time = 75.02 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.6839, time = 74.57 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.3539, time = 74.90 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.5245, time = 73.62 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.8917, time = 73.15 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.7644, time = 73.51 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 2.0313, time = 74.02 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 2.1556, time = 73.49 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.6769, time = 73.50 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.8778, time = 73.83 secondes ___
INFO:root:
 ******** Running time this step..1021.6721551418304
INFO:root:
*** avg_loss : 1.76, time : ~17.0 min (1021.67 sec) ***

INFO:root:==> evaluation : avg_loss = 1.84, time : 49.07 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 2 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.0286, time = 0.22 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 2.2042, time = 74.22 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.7488, time = 73.05 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.8728, time = 74.46 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 2.0601, time = 75.52 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.7075, time = 75.38 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.7006, time = 75.48 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.8033, time = 75.61 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 2.0135, time = 75.36 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.6717, time = 75.51 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.7753, time = 75.03 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 2.0172, time = 74.36 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.8656, time = 75.74 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.7952, time = 75.40 secondes ___
INFO:root:
 ******** Running time this step..1029.1023383140564
INFO:root:
*** avg_loss : 1.83, time : ~17.0 min (1029.10 sec) ***

INFO:root:==> evaluation : avg_loss = 1.85, time : 48.92 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 3 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.4777, time = 0.17 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.8360, time = 74.60 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.7305, time = 75.46 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.7218, time = 75.68 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.7662, time = 75.65 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.7649, time = 73.97 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.8423, time = 73.81 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.9291, time = 73.73 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.7401, time = 76.65 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.7747, time = 76.09 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.6129, time = 76.19 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 1.6373, time = 76.28 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 2.0136, time = 76.60 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.6700, time = 76.78 secondes ___
INFO:root:
 ******** Running time this step..1036.6998159885406
INFO:root:
*** avg_loss : 1.83, time : ~17.0 min (1036.70 sec) ***

INFO:root:==> evaluation : avg_loss = 1.85, time : 48.95 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 4 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.0465, time = 0.17 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.6017, time = 76.60 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 2.0246, time = 76.57 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 2.0968, time = 76.75 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.6512, time = 76.66 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.8749, time = 76.07 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.9026, time = 76.41 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.9030, time = 74.26 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.9476, time = 74.19 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.7754, time = 73.92 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.6587, time = 73.71 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 1.7990, time = 75.54 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.9343, time = 75.33 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.7258, time = 76.11 secondes ___
INFO:root:
 ******** Running time this step..1036.36035323143
INFO:root:
*** avg_loss : 1.83, time : ~17.0 min (1036.36 sec) ***

INFO:root:==> evaluation : avg_loss = 1.84, time : 48.84 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 5 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.1463, time = 0.17 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.8368, time = 75.22 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.7170, time = 75.80 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 2.0454, time = 75.66 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.8968, time = 73.80 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.5463, time = 73.01 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.7216, time = 73.14 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.6719, time = 73.37 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.7875, time = 74.02 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.8292, time = 73.38 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.9005, time = 75.88 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 1.9194, time = 75.40 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.5856, time = 73.42 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.8585, time = 73.13 secondes ___
INFO:root:
 ******** Running time this step..1017.5227932929993
INFO:root:
*** avg_loss : 1.82, time : ~16.0 min (1017.52 sec) ***

INFO:root:==> evaluation : avg_loss = 1.84, time : 48.92 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 6 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.8778, time = 0.22 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.9808, time = 73.43 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.7859, time = 75.74 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.7332, time = 75.96 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.6518, time = 75.74 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.8678, time = 75.59 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.9994, time = 75.76 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 2.1148, time = 76.18 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.8495, time = 76.08 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 2.0485, time = 76.20 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.4699, time = 75.87 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 1.7301, time = 75.95 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.9139, time = 76.05 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.9557, time = 75.57 secondes ___
INFO:root:
 ******** Running time this step..1037.0554399490356
INFO:root:
*** avg_loss : 1.83, time : ~17.0 min (1037.06 sec) ***

INFO:root:==> evaluation : avg_loss = 1.85, time : 48.94 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 7 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.2213, time = 0.16 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.8642, time = 74.09 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.8881, time = 74.37 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.8108, time = 74.33 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.9860, time = 74.77 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.8229, time = 75.40 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.7829, time = 73.78 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.7739, time = 75.23 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.9770, time = 75.84 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.9950, time = 74.88 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.9056, time = 74.23 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 1.8768, time = 75.35 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.8312, time = 76.19 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.7652, time = 76.20 secondes ___
INFO:root:
 ******** Running time this step..1029.1628403663635
INFO:root:
*** avg_loss : 1.83, time : ~17.0 min (1029.16 sec) ***

INFO:root:==> evaluation : avg_loss = 1.85, time : 48.88 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 8 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.6649, time = 0.17 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.7702, time = 76.24 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.9620, time = 76.11 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.9714, time = 76.19 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.7389, time = 76.15 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.9188, time = 76.33 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.6650, time = 77.66 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.6642, time = 76.22 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.9960, time = 76.43 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.9126, time = 76.31 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.4488, time = 76.30 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 2.0034, time = 76.28 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.8094, time = 75.68 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.9211, time = 74.39 secondes ___
INFO:root:
 ******** Running time this step..1044.6993753910065
INFO:root:
*** avg_loss : 1.83, time : ~17.0 min (1044.70 sec) ***

INFO:root:==> evaluation : avg_loss = 1.84, time : 48.88 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 9 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 1.8700, time = 0.17 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.7416, time = 76.24 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.7563, time = 75.46 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.6599, time = 74.53 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.5287, time = 75.56 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.6860, time = 75.45 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.5799, time = 74.90 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.8309, time = 74.24 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.6573, time = 74.06 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.8509, time = 74.59 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.6933, time = 76.63 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 1.6386, time = 76.02 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.7453, time = 75.95 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.8834, time = 76.05 secondes ___
INFO:root:
 ******** Running time this step..1034.117728471756
INFO:root:
*** avg_loss : 1.83, time : ~17.0 min (1034.12 sec) ***

INFO:root:==> evaluation : avg_loss = 1.84, time : 48.85 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:
=============== EPOCH 10 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.3195, time = 0.17 secondes ___
INFO:root:___ batch index = 500 / 6857 (7.29%), loss = 1.7578, time = 76.60 secondes ___
INFO:root:___ batch index = 1000 / 6857 (14.58%), loss = 1.5273, time = 76.30 secondes ___
INFO:root:___ batch index = 1500 / 6857 (21.88%), loss = 1.6523, time = 74.50 secondes ___
INFO:root:___ batch index = 2000 / 6857 (29.17%), loss = 1.8644, time = 76.08 secondes ___
INFO:root:___ batch index = 2500 / 6857 (36.46%), loss = 1.5086, time = 76.31 secondes ___
INFO:root:___ batch index = 3000 / 6857 (43.75%), loss = 1.8579, time = 76.54 secondes ___
INFO:root:___ batch index = 3500 / 6857 (51.04%), loss = 1.9098, time = 76.09 secondes ___
INFO:root:___ batch index = 4000 / 6857 (58.33%), loss = 1.5028, time = 73.56 secondes ___
INFO:root:___ batch index = 4500 / 6857 (65.63%), loss = 1.5715, time = 73.92 secondes ___
INFO:root:___ batch index = 5000 / 6857 (72.92%), loss = 1.9775, time = 73.84 secondes ___
INFO:root:___ batch index = 5500 / 6857 (80.21%), loss = 1.7167, time = 74.97 secondes ___
INFO:root:___ batch index = 6000 / 6857 (87.50%), loss = 1.7638, time = 75.85 secondes ___
INFO:root:___ batch index = 6500 / 6857 (94.79%), loss = 1.7795, time = 76.26 secondes ___
INFO:root:
 ******** Running time this step..1034.0888652801514
INFO:root:
*** avg_loss : 1.83, time : ~17.0 min (1034.09 sec) ***

INFO:root:==> evaluation : avg_loss = 1.85, time : 49.00 sec

INFO:root:=====>	{'accuracy': 0.3459743290548425, 'nb exemple': 3428, 'true_prediction': 1186, 'false_prediction': 2242}
INFO:root:	§§ model has been saved §§
INFO:root:

$$$$ average running time per epoch (sec)..1032.0482832431794
INFO:root:****************************************
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:****************************************
INFO:root:loading weights file pretrained_models/longformer-base-4096/pytorch_model.bin
INFO:root:Some weights of the model checkpoint at pretrained_models/longformer-base-4096 were not used when initializing HieformerForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).
- This IS NOT expected if you are initializing HieformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
INFO:root:Some weights of HieformerForSequenceClassification were not initialized from the model checkpoint at pretrained_models/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:root:
=============== EPOCH 1 / 10 ===============

INFO:root:___ batch index = 0 / 6857 (0.00%), loss = 2.1415, time = 0.72 secondes ___
